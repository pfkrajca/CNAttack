# -*- coding: utf-8 -*-
"""Final project functions only

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_TtpM4WfnBvZ6mUOVzl0JAG7S8JWTyOP

#**Finding Reference Cluster**
"""

import os, sys
os.environ['R_HOME'] = sys.exec_prefix+"/lib/R/"
import scanpy as sc
import scFates as scf
import numpy as np
import pandas as pd
from anndata import AnnData
from typing import Dict, Tuple
from hmmlearn import hmm
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
import warnings
import matplotlib.pyplot as plt
from scipy.sparse import issparse

# plot expression of gene expression to find reference cluster

def plot_cluster_means_by_genomic_position(adata, layer="counts", downsample=100):
    """
    Plots mean expression across genomic positions for each cell type in `adata`.

    Parameters:
    - adata: AnnData object with `layers[layer]` and `obs["cell_type"]` defined.
    - layer: Layer name containing count data.
    - downsample: Plot every nth gene to reduce visual clutter.
    """
    gene_indices = np.arange(0, adata.n_vars, downsample)

    # Get chromosome info if available
    chromosomes = adata.var["chromosome"].values[gene_indices] if "chromosome" in adata.var else None

    if chromosomes is not None:
        valid_chroms = [str(i) for i in range(1, 23)] + ['X', 'chrX']
        chrom_mask = np.isin(chromosomes, valid_chroms)
        chrom_changes = np.where((chromosomes[:-1] != chromosomes[1:]) &
                                 (chrom_mask[:-1] | chrom_mask[1:]))[0] + 1
        chrom_boundaries = [0] + chrom_changes.tolist() + [len(chromosomes)]
        chrom_midpoints = [(chrom_boundaries[i] + chrom_boundaries[i+1]) // 2
                           for i in range(len(chrom_boundaries)-1)]
        chrom_labels = [chromosomes[i] for i in chrom_boundaries[:-1]]

    # Calculate global Y-axis limits
    all_means = []
    for cluster in adata.obs["cell_type"].unique():
        cluster_mask = adata.obs["cell_type"] == cluster
        cluster_data = adata[cluster_mask].layers[layer][:, gene_indices]
        if issparse(cluster_data):
            cluster_data = cluster_data.toarray()
        all_means.append(np.mean(cluster_data, axis=0))

    global_ymin = min(np.min(means) for means in all_means)
    global_ymax = max(np.max(means) for means in all_means)
    y_padding = (global_ymax - global_ymin) * 0.1

    # 1. Combined Plot
    plt.figure(figsize=(14, 6))
    for cluster, means in zip(adata.obs["cell_type"].unique(), all_means):
        plt.plot(means, label=f"{cluster}", linewidth=1)

    if chromosomes is not None:
        for x in chrom_boundaries[1:-1]:
            if chrom_mask[x]:
                plt.axvline(x, color='gray', linestyle='--', alpha=0.5, linewidth=0.5)
        plt.xticks(
            [mp for mp, lbl in zip(chrom_midpoints, chrom_labels) if lbl in valid_chroms],
            [lbl for lbl in chrom_labels if lbl in valid_chroms],
            rotation=90
        )

    plt.ylim(global_ymin - y_padding, global_ymax + y_padding)
    plt.title(f"All Clusters (Downsampled {downsample}x)")
    plt.xlabel("Genomic Position")
    plt.ylabel("Mean Counts")
    plt.legend(bbox_to_anchor=(1.05, 1))
    plt.tight_layout()
    plt.show()

    # 2. Individual Cluster Plots
    for cluster, means in zip(adata.obs["cell_type"].unique(), all_means):
        plt.figure(figsize=(14, 4))
        plt.plot(means, color='steelblue', linewidth=1)

        if chromosomes is not None:
            for x in chrom_boundaries[1:-1]:
                if chrom_mask[x]:
                    plt.axvline(x, color='gray', linestyle='--', alpha=0.5, linewidth=0.5)
            plt.xticks(
                [mp for mp, lbl in zip(chrom_midpoints, chrom_labels) if lbl in valid_chroms],
                [lbl for lbl in chrom_labels if lbl in valid_chroms],
                rotation=90
            )

        plt.ylim(global_ymin - y_padding, global_ymax + y_padding)
        plt.title(f"Cluster: {cluster}\n(Mean Expression, {downsample}x downsampled)")
        plt.xlabel("Genomic Position")
        plt.ylabel("Mean Counts")
        plt.tight_layout()
        plt.show()

"""#**Smoothing (2)**"""

def order_genes_by_position(adata: AnnData) -> AnnData:
    # internal helper
    """
    Orders the variables (genes) in the AnnData object by chromosome and start.
    """
    gene_order = (
        adata.var
        .sort_values(['chromosome', 'start'])
        .index
    )
    return adata[:, gene_order]


def smooth_expression_matrix(matrix: pd.DataFrame, window_size: int = 25) -> pd.DataFrame:
    # internal helper
    """
    Applies a sliding window average across genes.
    """
    smoothed = []
    gene_names = matrix.columns.to_list()

    for i in range(len(gene_names) - window_size + 1):
        window = gene_names[i:i + window_size]
        avg_expr = matrix[window].mean(axis=1)
        smoothed.append(avg_expr.rename(f"{window[0]}_to_{window[-1]}"))

    return pd.concat(smoothed, axis=1)



def compute_smoothed_profiles_from_adata(
    adata: AnnData,
    use_layer: str = None,
    window_size: int = 25,
    ref: str = 'Monocyte'
) -> Tuple[pd.DataFrame, Dict[str, pd.Series], pd.Series, pd.Series]:
    # public function that helps users
    """
    Computes smoothed expression profiles from an AnnData object.
    Returns:
    - Per-cell smoothed expression DataFrame
    - Dictionary of per-cell-type smoothed average Series
    - Global average Series
    - Reference cluster average Series
    """
    # Step 1: Order genes
    adata_ordered = order_genes_by_position(adata)

    # Step 2: Get expression matrix
    if use_layer:
        expr = pd.DataFrame(
            adata_ordered.layers[use_layer].toarray(),
            index=adata_ordered.obs_names,
            columns=adata_ordered.var_names
        )
    else:
        expr = pd.DataFrame(
            adata_ordered.X.toarray() if hasattr(adata_ordered.X, "toarray") else adata_ordered.X,
            index=adata_ordered.obs_names,
            columns=adata_ordered.var_names
        )

    # Step 3: Smooth once for all cells
    smoothed_expr = smooth_expression_matrix(expr, window_size)

    # Step 4: Global average
    global_avg = smoothed_expr.mean(axis=0)

    # Step 5: Per-cell-type averages (vectorized)
    celltypes = adata_ordered.obs['cell_type']
    celltype_profiles = {
        ct: smoothed_expr.loc[celltypes == ct].mean(axis=0)
        for ct in celltypes.unique()
    }

    # Step 6: Reference cluster average
    if ref in celltype_profiles:
        ref_avg = celltype_profiles[ref]
    else:
        raise ValueError(f"Reference cell type '{ref}' not found in adata.obs['cell_type'].")

    return smoothed_expr, celltype_profiles, global_avg, ref_avg

def compute_all_cell_zscores_to_adata_optimized(smoothed: pd.DataFrame,
                                                celltype_avg: dict,
                                                global_avg: pd.Series,
                                                cluster_avg: pd.Series,
                                                adata: AnnData) -> AnnData:
    """
    Optimized memory-efficient version to compute z-score deltas and store them in `adata.obsm`.

    will store as adata.obsm['delta_global_z'], adata.obsm['delta_celltype_z'], adata.obsm['delta_cluster_z']
    """

    cells = smoothed.index
    genes = smoothed.columns
    n_cells, n_genes = smoothed.shape

    # Precompute arrays for faster access
    smoothed_np = smoothed.values
    global_avg_np = global_avg.values
    cluster_avg_np = cluster_avg.values

    # Build a matrix of per-cell-type averages aligned to cells
    cell_types = adata.obs.loc[cells, 'cell_type'].values
    celltype_avg_np = np.stack([celltype_avg[ct].loc[genes].values for ct in cell_types])

    # Allocate arrays
    delta_global_z = np.zeros_like(smoothed_np)
    delta_celltype_z = np.zeros_like(smoothed_np)
    delta_cluster_z = np.zeros_like(smoothed_np)

    # Helper function: z-score along gene axis (per cell)
    def zscore_rowwise(matrix):
        mean = matrix.mean(axis=1, keepdims=True)
        std = matrix.std(axis=1, keepdims=True)
        std[std == 0] = 1  # prevent division by zero
        return (matrix - mean) / std

    # Compute deltas
    delta_global = smoothed_np - global_avg_np
    delta_celltype = smoothed_np - celltype_avg_np
    delta_cluster = smoothed_np - cluster_avg_np

    # Z-score normalize each row (cell)
    delta_global_z = zscore_rowwise(delta_global)
    delta_celltype_z = zscore_rowwise(delta_celltype)
    delta_cluster_z = zscore_rowwise(delta_cluster)

    # Store results in adata.obsm
    adata.obsm['delta_global_z'] = delta_global_z
    adata.obsm['delta_celltype_z'] = delta_celltype_z
    adata.obsm['delta_cluster_z'] = delta_cluster_z
    adata.uns['delta_zscore_genes'] = list(genes)

    return adata

def filter_and_count_zscores(
    zscore_array,
    upper_thresh: float = 1.0,
    lower_thresh: float = -1.0,
    min_cells: int = 600,
    adata: AnnData = None,
    obsm_key: str = None
) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:
    """
    Robust z-score filtering with proper AnnData alignment

    Parameters:
        zscore_array: Input array (cells × genes)
        upper_thresh/lower_thresh: Significance thresholds
        min_cells: Minimum cells with significant changes
        adata: AnnData object (must match zscore_array rows)
        obsm_key: Key for storing filtered matrix in adata.obsm

    Returns:
        Tuple of (filtered_df, up_counts, down_counts)
    """

    # Validate dimensions
    if adata is not None:
        if zscore_array.shape[0] != adata.n_obs:
            raise ValueError(f"zscore_array has {zscore_array.shape[0]} cells, adata has {adata.n_obs}")

    # Convert to DataFrame with proper gene names
    zscore_df = pd.DataFrame(
        zscore_array,
        index=adata.obs_names if adata else None,
        columns=adata.var_names[:zscore_array.shape[1]] if adata else None
    )

    # Create significance masks
    sig_mask = (zscore_df > upper_thresh) | (zscore_df < lower_thresh)
    sig_counts = sig_mask.sum(axis=0)  # Count per gene

    # Filter genes
    keep_genes = sig_counts >= min_cells
    filtered_df = zscore_df.where(sig_mask & keep_genes, None)

    # Store in AnnData if requested
    if adata is not None and obsm_key is not None:
        # Create full-sized matrix aligned with adata.var
        filtered_matrix = np.zeros((adata.n_obs, adata.n_vars))
        filtered_matrix[:, :zscore_array.shape[1]] = filtered_df.values
        adata.obsm[obsm_key] = filtered_matrix

        # Store which genes were kept
        adata.uns[f"{obsm_key}_genes"] = filtered_df.columns.tolist()

    # Count significant events
    up_counts = (zscore_df > upper_thresh).sum(axis=0)
    down_counts = (zscore_df < lower_thresh).sum(axis=0)

    print(f"Filtered to {keep_genes.sum()} genes with changes in ≥{min_cells} cells")
    return filtered_df.loc[:, keep_genes], up_counts, down_counts

"""#**HMM modeling and CNVs**"""

def detect_cnvs_with_hmm_final(adata, matrix_name="filtered_z", n_components=3,
                             n_iter=50, random_state=42, output_prefix="hmm_cnv",
                             min_non_nan=20, chunk_size=100):
    """
    Final robust HMM implementation that:
    1. Uses pre-initialized model parameters
    2. Handles sparse data gracefully
    3. Provides detailed error reporting
    4. Stores per-gene HMM state and posterior in adata.var
    """

    if matrix_name not in adata.obsm:
        raise ValueError(f"Matrix '{matrix_name}' not found in adata.obsm")

    data = adata.obsm[matrix_name]
    n_cells, n_features = data.shape

    all_states = np.full(n_features, -1, dtype=np.int8)
    all_posteriors = np.full((n_features, n_components), np.nan, dtype=np.float16)

    print("Identifying valid features...")
    valid_features = [f for f in range(n_features) if np.sum(~np.isnan(data[:, f])) >= min_non_nan]

    if len(valid_features) < n_components:
        raise ValueError(f"Only {len(valid_features)} valid features found - need at least {n_components} for HMM")

    # Initialize HMM model
    model = hmm.GaussianHMM(
        n_components=n_components,
        covariance_type="diag",
        n_iter=0,
        init_params="",
        means_prior=np.linspace(-2, 2, n_components).reshape(-1, 1),
        covars_prior=np.ones((n_components, 1)),
        transmat_prior=np.full((n_components, n_components), 1/n_components),
        random_state=random_state
    )

    model_fitted = False

    print("Processing in chunks...")
    for chunk_start in tqdm(range(0, len(valid_features), chunk_size)):
        chunk_features = valid_features[chunk_start:chunk_start + chunk_size]
        chunk = data[:, chunk_features]

        valid_cells = ~np.isnan(chunk).any(axis=1)
        clean_chunk = chunk[valid_cells, :]

        if clean_chunk.shape[0] < min_non_nan:
            continue

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            scaled_chunk = StandardScaler().fit_transform(clean_chunk)

        if not model_fitted:
            try:
                model.n_iter = n_iter
                model.fit(scaled_chunk.T)
                model_fitted = True
                print("Model successfully fitted!")
            except Exception as e:
                print(f"Fit failed: {str(e)}")
                continue

        try:
            states = model.predict(scaled_chunk.T)
            posteriors = model.predict_proba(scaled_chunk.T)

            for i, f in enumerate(chunk_features):
                all_states[f] = states[i]
                all_posteriors[f] = posteriors[i].astype(np.float16)
        except Exception as e:
            print(f"Prediction failed for chunk {chunk_start}: {str(e)}")
            continue

    if not model_fitted:
        raise RuntimeError("Could not fit HMM to any data chunks")

    # Store in adata.var
    adata.var[f"{output_prefix}_state"] = -1
    for i in range(n_components):
        adata.var[f"{output_prefix}_prob_state_{i}"] = np.nan

    for idx, gene in enumerate(adata.var_names):
        adata.var.loc[gene, f"{output_prefix}_state"] = int(all_states[idx])
        for i in range(n_components):
            adata.var.loc[gene, f"{output_prefix}_prob_state_{i}"] = all_posteriors[idx, i]

    print("Stored HMM states and posteriors in adata.var")
    return adata

def format_detected_cnvs_with_cell_counts(adata, state_col='hmm_cnv_state', output_col='detected_cnvs',
                                         chrom_col='chromosome', start_col='start', end_col='end',
                                         max_gap=1e6, min_region_size=1000, z_threshold=1.5):
    """
    Enhanced version that:
    1. Merges overlapping CNV regions
    2. Counts how many cells contain each CNV pattern
    3. Stores both region counts and cell counts
    """
    import pandas as pd
    import numpy as np
    from scipy.stats import zscore

    # Custom mapping based on your state distribution
    state_to_cn = {
        -1: None,  # Will be filtered out
        0: '0',    # Deletion
        1: '2',    # Neutral
        2: '4'     # Amplification
    }

    # Filter out invalid features first
    valid_features = adata.var[state_col] >= 0  # Only keep states 0,1,2
    sorted_vars = adata.var[valid_features].sort_values([chrom_col, start_col])

    if len(sorted_vars) == 0:
        print("No valid CNV regions found!")
        adata.obs[output_col] = "neutral"
        return adata, pd.DataFrame()

    # Convert to list of regions for easier processing
    regions = []
    for _, row in sorted_vars.iterrows():
        cn = state_to_cn[row[state_col]]
        if cn is None:
            continue
        regions.append({
            'chrom': row[chrom_col],
            'start': float(row[start_col]),
            'end': float(row[end_col]),
            'cn': cn,
            'genes': [row.name]  # Track which genes are in this region
        })

    # Merge overlapping regions (overlapping genomic regions are counted as one CNV)
    merged = []
    current = None

    for region in sorted(regions, key=lambda x: (x['chrom'], x['start'])):
        if current is None:
            current = region.copy()
            continue

        # Same chromosome and CN state, and either overlapping or within max_gap
        if (region['chrom'] == current['chrom'] and
            region['cn'] == current['cn'] and
            region['start'] <= current['end'] + max_gap):

            # Merge the regions
            current['end'] = max(current['end'], region['end'])
            current['genes'].extend(region['genes'])
        else:
            # Save current region if large enough
            if current['end'] - current['start'] >= min_region_size:
                merged.append(current)
            # Start new region
            current = region.copy()

    # Add the last region if valid
    if current and current['end'] - current['start'] >= min_region_size:
        merged.append(current)

    if not merged:
        print("No valid CNV regions after merging!")
        adata.obs[output_col] = "neutral"
        return adata, pd.DataFrame()

    # Create formatted strings and count cells
    results = []
    cell_cnv_matrix = np.zeros((adata.n_obs, len(merged)), dtype=bool)

    for i, region in enumerate(merged):
        # Get expression for genes in this region
        expr = adata[:, region['genes']].X
        if hasattr(expr, 'toarray'):
            expr = expr.toarray()

        # Calculate z-scores for this region across cells
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            z_scores = zscore(expr, axis=0, nan_policy='omit')
            region_z = np.nanmean(z_scores, axis=1)

        # Determine which cells have this CNV
        if region['cn'] == '0':  # Deletion
            cnv_cells = region_z < -z_threshold
        elif region['cn'] == '4':  # Amplification
            cnv_cells = region_z > z_threshold
        else:
            cnv_cells = np.zeros(adata.n_obs, dtype=bool)

        cell_cnv_matrix[:, i] = cnv_cells

        # Format region string
        region_str = f"{region['chrom']}:{int(region['start'])}-{int(region['end'])} (CN {region['cn']})"
        n_cells = np.sum(cnv_cells)

        results.append({
            'region': region_str,
            'n_cells': n_cells,
            'genes': ','.join(region['genes'])
        })

    # Create DataFrame with region counts and cell counts
    cnv_df = pd.DataFrame(results).sort_values('n_cells', ascending=False)
    cnv_df['percent_cells'] = (cnv_df['n_cells'] / adata.n_obs * 100).round(1)

    # Assign CNV patterns to cells
    adata.obs[output_col] = "neutral"
    for i in range(len(merged)):
        region_str = cnv_df.iloc[i]['region']
        adata.obs.loc[cell_cnv_matrix[:, i], output_col] = region_str

    # For cells with multiple CNVs, concatenate patterns
    multi_cnv_cells = np.sum(cell_cnv_matrix, axis=1) > 1
    if np.any(multi_cnv_cells):
        for cell_idx in np.where(multi_cnv_cells)[0]:
            patterns = [cnv_df.iloc[i]['region']
                       for i in np.where(cell_cnv_matrix[cell_idx])[0]]
            adata.obs.iloc[cell_idx, adata.obs.columns.get_loc(output_col)] = "; ".join(patterns)

    print(f"Detected {len(merged)} merged CNV regions across {np.sum(cell_cnv_matrix)} cell-region pairs")
    print("\nTop CNV regions by cell count:")
    print(cnv_df.head())

    # Store additional information
    adata.uns['cnv_stats'] = {
        'total_regions': len(merged),
        'total_cell_cnv_pairs': np.sum(cell_cnv_matrix),
        'cells_with_cnvs': np.sum(adata.obs[output_col] != "neutral"),
        'cell_cnv_matrix': cell_cnv_matrix,
        'merged_regions': merged
    }

    return adata, cnv_df
